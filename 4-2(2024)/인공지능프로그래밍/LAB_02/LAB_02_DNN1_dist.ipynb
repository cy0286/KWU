{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Programming - SW Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 02: Simple Deep Neural Network\n",
    "## Exercise: Predicting MNIST Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhC9a_u5Ta4u"
   },
   "source": [
    "### Prepare Mini-MNIST Dataset from Scikit-Learn\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "MboBxtwcTa41"
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "# digits.data from sklearn contains 1797 images of 8x8 pixels\n",
    "# Each image has a hand-written digit\n",
    "digits_df = digits.images.reshape((len(digits.target), -1))\n",
    "digits_tf = digits.target\n",
    "\n",
    "# Splitting dataframe into train & test\n",
    "X_train_org, X_test_org, y_train_num, y_test = train_test_split(digits_df, digits_tf, test_size= 0.20, random_state= 101)\n",
    "\n",
    "# Digits data has range of [0,16], which often lead too big exponential values\n",
    "# so make them normal distribution of [0,1] with the sklearn package, or you can just divide them by 16\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train_org)\n",
    "X_test = sc.transform(X_test_org)\n",
    "\n",
    "n_classes = 10\n",
    "\n",
    "# Transform Nx1 Y vector into Nx10 answer vector, so that we can perform one-to-all classification\n",
    "y_train = np.zeros((y_train_num.shape[0],10))\n",
    "for i in range(n_classes):\n",
    "    y_train[:,i] = (y_train_num == i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function can be defined as:\n",
    "\n",
    "$$ \\text{sigmoid}(x) = {1 \\over {1 + e^{-x}}} = {e^{x} \\over {1 + e^{x}}} $$\n",
    "\n",
    "Sigmoid function takes numbers between $[-\\infty, \\infty]$ and gives back numbers between $[0, 1]$.<br>\n",
    "However, the corresponding numpy implementation warns overflow for large negative inputs.<br>\n",
    "The function below is the implementation of numerically stable sigmoid. Complete the code **without using `if` statement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mySigmoid(x):\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    positive = (x >= 0) # Create a boolean array indicating which elements in x are positive \n",
    "    x_p = x[positive]   # Extract the positive values from x\n",
    "    x_n = x[~positive]  # Extract the negative values from x\n",
    "    x[positive] = 1 / (1 + np.exp(-x_p))            # Apply the sigmoid function to the positive values\n",
    "    x[~positive] = np.exp(x_n) / (1 + np.exp(x_n))  # Apply the sigmoid function to the negative values\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. , 0. ])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySigmoid(np.array([0.0, 1000.0, -1000.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax function can be defined as:\n",
    "\n",
    "$$ \\text{softmax}(x_i) = {e^{x_i} \\over {\\sum_{k=1}^n e^{x_k}}} $$\n",
    "\n",
    "Softmax function also takes numbers between $[-\\infty, \\infty]$ and gives back numbers between $[0, 1]$.<br>\n",
    "Therefore, this function has the same overflow problem for large positive inputs.<br>\n",
    "The function below is the implementation of numerically stable softmax.<br>\n",
    "You can make the softmax stable by multiplying $e^{-M}$ to both numerator and denominator. <br>\n",
    "Complete the code **without using `if` statement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define softmax. Assume (b, s)\n",
    "def mySoftmax(x):\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    x = x - np.max(x, axis = -1, keepdims = True)        # Subtract the max value from each row for make x sufficiently small\n",
    "    x = np.exp(x)                                        # execute exponential function\n",
    "    x = x / np.sum(x, axis = -1, keepdims = True)        # calculate softmax\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mySoftmax(np.array([0.0, 1000.0, -1000.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1437, 64)\n",
      "(1437, 10)\n",
      "[ 0.  0.  0.  9. 16.  6.  0.  0.  0.  0.  4. 15.  6. 15.  0.  0.  0.  0.\n",
      "  8. 11.  9. 11.  0.  0.  0.  0.  8. 16. 14.  2.  0.  0.  0.  0. 11. 16.\n",
      " 13.  0.  0.  0.  0.  6. 14.  2. 12.  9.  0.  0.  0.  5. 16. 11.  5. 13.\n",
      "  4.  0.  0.  0.  3.  8. 13. 16.  9.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number is 3\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the digits DataFrame and the training data\n",
    "print(digits_df.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "# Print the first sample from the original training data\n",
    "print(X_train_org[0])\n",
    "\n",
    "idx = np.random.randint(X_train.shape[0]) # Randomly select an index from the training data\n",
    "dimage = X_train_org[idx].reshape((8,8))  # Reshape the selected sample\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.gray()\n",
    "plt.matshow(dimage, fignum=1)\n",
    "plt.show()\n",
    "print('The number is', y_train_num[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple DNN for Digit Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDenseLayer:\n",
    "    def __init__(self, n_out, n_in):\n",
    "        # Initialize the weights and biases\n",
    "        self.wegt = np.empty((n_out, n_in))\n",
    "        self.bias = np.zeros((n_out))\n",
    "        self.saved_x = None     # store x to use while backpropagation\n",
    "\n",
    "    def forward(self, x):       # (b, i)\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        self.saved_x = x                                          # Save x for use in backpropagation\n",
    "        x_lin = (np.matmul(self.wegt, x.T).T) + self.bias         # Linear Prediction\n",
    "        ### END CODE HERE ###\n",
    "        return x_lin\n",
    "\n",
    "    def backward(self, x, x_in):  # x = dJ/dz (b, c)\n",
    "        # Check if input during forward pass matches the one during backpropagation\n",
    "        assert np.array_equal(self.saved_x, x_in), print('x_in does not equal to input X.')\n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        dw = np.matmul(x.T, x_in)         # Gradients for weights\n",
    "        db = np.sum(x, axis = 0)          # Gradients for biases\n",
    "        wdJdz = np.matmul(x, self.wegt)   # Propagation for lower layer\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return dw, db, wdJdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.23890168  3.05091188 -3.32627831]\n",
      "  [ 0.388114    3.36724875  1.06158492]\n",
      "  [ 3.10267869  1.87570497 -1.8326582 ]]\n",
      "\n",
      " [[-7.60581826  2.36703751 -1.16423539]\n",
      "  [ 3.48035012  2.41940644 -0.13917734]\n",
      "  [ 1.20541315  2.07585619 -1.5435161 ]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "tmp = myDenseLayer(3,5)\n",
    "tmp.wegt = np.random.randn(3,5)\n",
    "tmp.bias = np.random.randn(3)\n",
    "\n",
    "print(tmp.forward(np.random.randn(2,5,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outputs**\n",
    "\n",
    "```\n",
    "[[[ 3.23890168  3.05091188 -3.32627831]\n",
    "  [ 0.388114    3.36724875  1.06158492]\n",
    "  [ 3.10267869  1.87570497 -1.8326582 ]]\n",
    "\n",
    " [[-7.60581826  2.36703751 -1.16423539]\n",
    "  [ 3.48035012  2.41940644 -0.13917734]\n",
    "  [ 1.20541315  2.07585619 -1.5435161 ]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Backpropagation of Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJdz_sigmoid(wdJdz_upper, az): # Backpropagation through the sigmoid activation function\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    dJdz = wdJdz_upper * (az * (1 - az))  # backpropagation through activation function\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return dJdz\n",
    "\n",
    "def dJdz_softmax(y_hat, y):       # Backpropagation through the softmax activation function\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    dJdz =  y_hat - y                     # backpropagation through activation function\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return dJdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.90531647 -0.64834065 -1.89126428]\n",
      "[ 0.53948992 -0.29540078 -1.55749236]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "print(dJdz_sigmoid(np.random.randn(3),np.random.randn(3)))\n",
    "print(dJdz_softmax(np.random.randn(3),np.random.randn(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outputs**\n",
    "\n",
    "```\n",
    "[-4.90531647 -0.64834065 -1.89126428]\n",
    "[ 0.53948992 -0.29540078 -1.55749236]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Functions<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_forward(l1, l2, l3, X_in): # Forward propagation through three layers\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    a_1 = mySigmoid(l1.forward(X_in))                 # first stage forward(sigmoid)\n",
    "    a_2 = mySigmoid(l2.forward(a_1))                  # second stage forward(sigmoid)\n",
    "    a_3 = mySoftmax(l3.forward(a_2))                  # third stage forward(softmax)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return a_1, a_2, a_3\n",
    "\n",
    "def my_backward(l1, l2, l3, a_1, a_2, a_3, X_in, y_true): # Backward propagation through three layers\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    dw_3, db_3, wdJdz_3 = l3.backward(dJdz_softmax(a_3, y_true), a_2)     # go through 3rd stage backward(softmax)\n",
    "    dw_2, db_2, wdJdz_2 = l2.backward(dJdz_sigmoid(wdJdz_3, a_2), a_1)    # go through 2nd stage backward(sigmoid)\n",
    "    dw_1, db_1, _       = l1.backward(dJdz_sigmoid(wdJdz_2, a_1), X_in)   # go through 1st stage backward(sigmoid)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Return gradients for all layers\n",
    "    d_1 = [dw_1, db_1]\n",
    "    d_2 = [dw_2, db_2]\n",
    "    d_3 = [dw_3, db_3]\n",
    "    \n",
    "    return d_1, d_2, d_3\n",
    "\n",
    "def my_loss(l1, l2, l3, X_in, y_true): # Calculate the loss\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    a_1,a_2,a_3 = my_forward(l1, l2, l3, X_in)                   # Forward pass to get the predictions\n",
    "    loss = -(np.sum(y_true * np.log(a_3))) / X_in.shape[0]       # calculate cross-entropy loss\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return loss\n",
    "    \n",
    "def my_predict(l1, l2, l3, X_in):     # Make predictions\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    a_1,a_2,a_3 = my_forward(l1, l2, l3, X_in)       # Forward pass to get the softmax probabilities\n",
    "    pred = np.argmax(a_3, axis=1)                    # make prediction\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a NN model and check the matrix dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "DT0rMw-rTa5A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (1437, 10)\n",
      "(80, 64) (80,)\n",
      "(70, 80) (70,)\n",
      "(10, 70) (10,)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of inputs, hidden units, and output classes\n",
    "n_inputs  = 64\n",
    "n_hidden1 = 80\n",
    "n_hidden2 = 70\n",
    "n_classes = 10\n",
    "\n",
    "# Initialize three layers of the neural network\n",
    "l1 = myDenseLayer(n_hidden1, n_inputs)\n",
    "l2 = myDenseLayer(n_hidden2, n_hidden1)\n",
    "l3 = myDenseLayer(n_classes, n_hidden2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(l1.wegt.shape, l1.bias.shape)\n",
    "print(l2.wegt.shape, l2.bias.shape)\n",
    "print(l3.wegt.shape, l3.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Outputs**\n",
    "\n",
    "```\n",
    "(1437, 64) (1437, 10)\n",
    "(80, 64) (80,)\n",
    "(70, 80) (70,)\n",
    "(10, 70) (10,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights are initialized to...\n",
    "l1.wegt = np.random.randn(n_hidden1, n_inputs)\n",
    "l2.wegt = np.random.randn(n_hidden2, n_hidden1)\n",
    "l3.wegt = np.random.randn(n_classes, n_hidden2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Simple Neural Network Model (3 layer model) (<b>Update weights</b>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55777,
     "status": "ok",
     "timestamp": 1649259680196,
     "user": {
      "displayName": "Seong-Won Lee",
      "userId": "14858304546124675216"
     },
     "user_tz": -540
    },
    "id": "qODinrZlTa5C",
    "outputId": "8237949a-964a-49cd-f3e9-fb65759245e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  500,  loss: 0.01596297\n",
      "Epoch: 1000,  loss: 0.00289776\n",
      "Epoch: 1500,  loss: 0.00176497\n",
      "Epoch: 2000,  loss: 0.00104026\n",
      "Epoch: 2500,  loss: 0.00081159\n",
      "Epoch: 3000,  loss: 0.00058899\n",
      "Epoch: 3500,  loss: 0.00046510\n",
      "Epoch: 4000,  loss: 0.00033567\n",
      "Epoch: 4500,  loss: 0.00027970\n",
      "Epoch: 5000,  loss: 0.00021400\n"
     ]
    }
   ],
   "source": [
    "# alpha: learning rate, lamda: regularization factor\n",
    "alpha = 0.01\n",
    "n_epochs = 5000\n",
    "\n",
    "for epoch in range(n_epochs): # Training loop\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Forward Path\n",
    "    a_1, a_2, a_3 = my_forward(l1, l2, l3, X_train)                             # Forward pass through the network\n",
    "    \n",
    "    # Backward Path\n",
    "    d_1, d_2, d_3 =  my_backward(l1, l2, l3, a_1, a_2, a_3, X_train, y_train)   # Backward pass to compute gradients for all layers\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    # Extract the gradients for weights and biases from each layer\n",
    "    dw_1, db_1 = d_1\n",
    "    dw_2, db_2 = d_2\n",
    "    dw_3, db_3 = d_3\n",
    "\n",
    "    # Update weights and biases\n",
    "    ### START CODE HERE ###\n",
    "    l3.wegt -= alpha * dw_3\n",
    "    l3.bias -= alpha * db_3\n",
    "    l2.wegt -= alpha * dw_2\n",
    "    l2.bias -= alpha * db_2\n",
    "    l1.wegt -= alpha * dw_1\n",
    "    l1.bias -= alpha * db_1\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print loss\n",
    "    if ((epoch+1)%500==0):\n",
    "        loss_J = my_loss(l1, l2, l3, X_train, y_train)\n",
    "        print('Epoch: %4d,  loss: %10.8f' % (epoch+1, loss_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1649259686353,
     "user": {
      "displayName": "Seong-Won Lee",
      "userId": "14858304546124675216"
     },
     "user_tz": -540
    },
    "id": "xMvLn6SJTa5D",
    "outputId": "229cafc3-c9c5-4dd2-f7bc-d4cdf90c4d46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test set using the trained model\n",
    "y_pred = my_predict(l1, l2, l3, X_test)\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WieUxPz9Ta5F"
   },
   "source": [
    "Neural Network from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3086,
     "status": "ok",
     "timestamp": 1649259694763,
     "user": {
      "displayName": "Seong-Won Lee",
      "userId": "14858304546124675216"
     },
     "user_tz": -540
    },
    "id": "w8AcitiaTa5G",
    "outputId": "09455c7b-a58f-4492-b5cb-431eab436f6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(80, 70, ), activation='logistic', solver='sgd', \\\n",
    "                    alpha=0.01, learning_rate_init=0.01, max_iter=1000)\n",
    "\n",
    "# Training/Fitting the Model\n",
    "mlp.fit(X_train, y_train_num)\n",
    "\n",
    "# Making Predictions\n",
    "s_pred = mlp.predict(X_test)\n",
    "accuracy_score(s_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmZQrDH9n0PK"
   },
   "source": [
    "### Test Model with a random sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My prediction is 3\n",
      "sk prediction is 3\n",
      "Actual number is 3\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(X_test.shape[0])\n",
    "dimage = X_test_org[idx].reshape((8,8))\n",
    "# Plot the image\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.gray()\n",
    "plt.matshow(dimage, fignum=1)\n",
    "plt.show()\n",
    "\n",
    "X_input = np.expand_dims(X_test[idx], 0)\n",
    "\n",
    "y_pred = my_predict(l1, l2, l3, X_input)\n",
    "\n",
    "s_pred = mlp.predict(X_input)\n",
    "\n",
    "print('My prediction is ' + str(y_pred[0]))\n",
    "print('sk prediction is ' + str(s_pred[0]))\n",
    "print('Actual number is ' + str(y_test[idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 2024 SW Lee"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML_L06_01_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "19c3f3f12223855b5e5811df3b51e2142b7f938327ffb9b9a66299337f7b60d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
